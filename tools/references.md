# üìö Refer√™ncias Bibliogr√°ficas & Benchmarks

> **A base epistemol√≥gica do Linguistic Laboratory Framework.**
> Esta curadoria re√∫ne os papers can√¥nicos, reposit√≥rios de c√≥digo e teorias f√≠sicas que fundamentam o **Rigor 1.0** e a **Semantic Latent Engineering (SLE)**.

---

### üèÜ Tier 1: O C√¢none (Leitura Obrigat√≥ria)

Estes trabalhos definem os axiomas operacionais do laborat√≥rio. Se est√° aqui, √© porque o m√©todo foi validado empiricamente e adotado pela ind√∫stria.

| Paper / Tecnologia | Cita√ß√£o / ArXiv | Relev√¢ncia para o Lab (SLE) | Status |
| :--- | :--- | :--- | :---: |
| **RepE (Representation Engineering)** | [Zou et al. (2023)](https://arxiv.org/abs/2310.01405) | Base do operador `>>` (Steering). Prova que podemos controlar IAs via vetores, n√£o apenas prompts. | üíé Core |
| **Reflexion** | [Shinn et al. (NeurIPS 2023)](https://arxiv.org/abs/2303.11366) | Base do ciclo de auto-corre√ß√£o e agentes aut√¥nomos verbais. | üíé Core |
| **RAGAS** | [Es et al. (EACL 2024)](https://arxiv.org/abs/2309.15217) | Framework padr√£o para m√©tricas de valida√ß√£o sem√¢ntica (Faithfulness, Relevance). | üíé Metric |
| **TransformerLens** | [Nanda & Meyer](https://github.com/TransformerLensOrg/TransformerLens) | A "lente" principal para mechanistic interpretability. | üõ†Ô∏è Tool |
| **DARE** | [Yu et al. (2023)](https://arxiv.org/abs/2311.03099) | Algoritmo de reconstru√ß√£o e *merging* de modelos sem retreino. | ‚ö° SOTA |
| **CoVe (Chain of Verification)** | [Dhuliawala et al. (2023)](https://arxiv.org/abs/2309.11495) | Mitiga√ß√£o de alucina√ß√£o via verifica√ß√£o passo-a-passo. | üõ°Ô∏è Safety |

---

### üåå 1. F√≠sica do Espa√ßo Latente (Geometria & Manifolds)

A teoria por tr√°s da "√Ålgebra da Inten√ß√£o". Como a informa√ß√£o se organiza geometricamente dentro da rede.

* **Linear Representations:** [The Linear Representation Hypothesis](https://arxiv.org/abs/2311.03658) ‚Äî *Por que vetores funcionam como conceitos.*
* **Monosemanticity:** [Scaling Monosemanticity (Anthropic)](https://transformer-circuits.pub/2024/scaling-monosemanticity/) ‚Äî *Como extrair conceitos puros (features) de redes neurais.*
* **Latent Space Physics:** [Papers sobre Din√¢mica de Manifolds](https://arxiv.org/html/2406.12159v1) ‚Äî *Estudos sobre a topologia do pensamento da IA.*
* **Emergent World Models:** [Othello-GPT & World Models](https://arxiv.org/abs/2210.13382) ‚Äî *Prova de que IAs constroem modelos de mundo internos.*

---

### ‚öôÔ∏è 2. Mec√¢nica da Infer√™ncia (Circuitos & Residual Stream)

Como o motor funciona "debaixo do cap√¥". Estudos de *Mechanistic Interpretability*.

* **Induction Heads:** [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) ‚Äî *A mec√¢nica biol√≥gica do "few-shot learning".*
* **Residual Stream:** [Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) ‚Äî *O fluxo de informa√ß√£o atrav√©s das camadas.*
* **AutoInterp:** [Automated Interpretability (EleutherAI)](https://blog.eleuther.ai/autointerp/) ‚Äî *Usando IA para explicar IA.*
* **ACDC:** [Automated Circuit DisCovery](https://arxiv.org/abs/2304.14997) ‚Äî *Mapeamento autom√°tico de circuitos computacionais.*

---

### üß™ 3. T√©cnicas SOTA (Steering & Controle)

M√©todos avan√ßados para guiar o comportamento (Behavior) sem retreino.

* **Activation Steering:** [Activation Addition/Steering](https://arxiv.org/abs/2308.10248) ‚Äî *Inje√ß√£o direta de vetores de comportamento.*
* **ITI (Inference-Time Intervention):** [Li et al. (2023)](https://arxiv.org/abs/2306.03341) ‚Äî *Cirurgia em tempo real para aumentar a veracidade.*
* **TIES-Merging:** [Resolving Interference when Merging Models](https://arxiv.org/abs/2306.01708) ‚Äî *Como combinar m√∫ltiplos modelos especialistas.*
* **ToT (Tree of Thoughts):** [Yao et al. (2023)](https://arxiv.org/abs/2305.10601) ‚Äî *Navega√ß√£o n√£o-linear de racioc√≠nio.*

---

### üìä 4. Valida√ß√£o Cient√≠fica & M√©tricas

Ferramentas e papers que fundamentam o nosso **Scientific Validation Hub**.

* **Probing Classifiers:** [Linear Probes for Concept Extraction](https://arxiv.org/abs/2102.12452) ‚Äî *A base matem√°tica do nosso SD Validator.*
* **Faithfulness Metrics:** [RAGAS Documentation & Metrics](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/) ‚Äî *Como medir se a IA manteve o contexto.*
* **Semantic Entropy:** [Kuhn et al. (2023)](https://arxiv.org/abs/2302.09664) ‚Äî *Detectando alucina√ß√£o atrav√©s da incerteza sem√¢ntica.*

---

### üõ†Ô∏è 5. Reposit√≥rios de Implementa√ß√£o (Open Source)

Onde o c√≥digo vive. Links diretos para implementa√ß√µes de refer√™ncia.

* **[TransformerLens](https://github.com/TransformerLensOrg/TransformerLens):** A biblioteca definitiva para an√°lise mecanicista.
* **[Pythia](https://github.com/EleutherAI/pythia):** Suite de modelos para pesquisa de interpretabilidade.
* **[Mechanistic Interpretability (Awesome List)](https://github.com/gauravfs-14/awesome-mechanistic-interpretability):** Curadoria da comunidade.
* **[PEFT (HuggingFace)](https://huggingface.co/docs/peft/package_reference/lora):** Parameter-Efficient Fine-Tuning (LoRA, etc.).

---

### üîç Notas de Curadoria

> **Crit√©rio de Inclus√£o:**
> Para constar nesta lista, o trabalho deve ter:
> 1.  C√≥digo aberto dispon√≠vel ou algoritmo replic√°vel.
> 2.  Valida√ß√£o emp√≠rica (Benchmarks) ou te√≥rica robusta.
> 3.  Relev√¢ncia direta para a engenharia de prompts ou agentes.

**√öltima Atualiza√ß√£o:** 2025-11-24
**Rigor Level:** 1.0 (Canonical Sources Only)
